# Observabilidade - Chat4All

## ğŸ“Š VisÃ£o Geral

Este documento descreve a implementaÃ§Ã£o completa de observabilidade no Chat4All, conforme **SeÃ§Ã£o 2.4** do `esqueleto.md`. O sistema expÃµe mÃ©tricas em formato Prometheus, dashboards Grafana para visualizaÃ§Ã£o, Circuit Breakers para resiliÃªncia, e scripts de load testing k6 para validaÃ§Ã£o de performance.

## ğŸ—ï¸ Arquitetura de Monitoramento

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Service â”‚â”€â”€â”€â”€â–¶â”‚ Prometheus  â”‚â”€â”€â”€â”€â–¶â”‚   Grafana   â”‚â”€â”€â”€â”€â–¶â”‚  Dashboards â”‚
â”‚ :8080       â”‚     â”‚ :9090       â”‚     â”‚ :3000       â”‚     â”‚             â”‚
â”‚ /actuator/  â”‚     â”‚  Scraper    â”‚     â”‚Visualizationâ”‚     â”‚ - Overview  â”‚
â”‚ prometheus  â”‚     â”‚  (10-15s)   â”‚     â”‚             â”‚     â”‚ - API       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ - Router    â”‚
       â”‚                                                     â”‚ - Connectorsâ”‚
       â”œâ”€â”€â”€ Router Worker :8082 (Kafka, Cassandra)         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”œâ”€â”€â”€ WhatsApp Connector :8083 (Circuit Breaker)
       â””â”€â”€â”€ Instagram Connector :8084 (Circuit Breaker)
```

## ğŸ¯ MÃ©tricas Implementadas

### API Service (`:8080/actuator/prometheus`)
- `http_requests_total{method, endpoint, status}` - Total de requisiÃ§Ãµes HTTP
- `http_request_duration_seconds{method, endpoint}` - LatÃªncia de requisiÃ§Ãµes
- `messages_accepted_total` - Mensagens aceitas pela API
- `messages_rejected_total{reason}` - Mensagens rejeitadas (auth_missing, auth_invalid, validation_failed)
- `files_uploaded_total` - Arquivos enviados
- `kafka_publish_duration_seconds` - LatÃªncia de publicaÃ§Ã£o no Kafka

### Router Worker (`:8082/actuator/prometheus`)
- `messages_consumed_total` - Mensagens consumidas do Kafka
- `kafka_consumer_lag` - Consumer lag (atraso no processamento)
- `processing_duration_seconds` - Tempo total de processamento
- `messages_processed_total{status}` - Mensagens processadas por status (SENT, ROUTED, DELIVERED, FAILED, DUPLICATE)
- `cassandra_write_duration_seconds` - LatÃªncia de escrita no Cassandra
- `messages_failed_total{reason}` - Falhas por razÃ£o (cassandra_error, processing_error, runtime_error)

### Conectores WhatsApp/Instagram (`:8083`, `:8084`)
- `messages_sent_total{channel, status}` - Mensagens enviadas por canal (whatsapp, instagram)
- `connector_api_duration_seconds{channel}` - LatÃªncia de chamadas API externa
- `circuit_breaker_state{channel}` - Estado do circuit breaker (0=CLOSED, 1=OPEN, 50=HALF_OPEN)
- `circuit_breaker_transitions_total{channel, from, to}` - TransiÃ§Ãµes de estado do circuit breaker

## ğŸš€ InÃ­cio RÃ¡pido

### 1. Demo Completo (Recomendado)

```bash
# Executa script automÃ¡tico que:
# - ConstrÃ³i aplicaÃ§Ã£o
# - Inicia todos os serviÃ§os
# - Executa health checks
# - Oferece executar baseline test
./scripts/demo-observability.sh
```

### 2. Acesso aos Componentes

| Componente | URL | Credenciais |
|------------|-----|-------------|
| Grafana | http://localhost:3000 | admin / admin |
| Prometheus | http://localhost:9090 | - |
| API Service | http://localhost:8080 | - |
| MinIO Console | http://localhost:9001 | minioadmin / minioadmin |

### 3. Dashboards Grafana

Navegue para **Dashboards â†’ Chat4All** folder:

1. **System Overview** - VisÃ£o geral do sistema
   - HTTP RPS total
   - Messages/sec aceitos
   - Consumer lag
   - Processing latency p95

2. **API Service** - MÃ©tricas HTTP e Kafka
   - Request rate por endpoint
   - HTTP latency p95
   - Message acceptance/rejection rate
   - Kafka publish latency (p50, p95, p99)

3. **Router Worker** - Processamento de mensagens
   - Kafka consumer lag (gauge com thresholds)
   - Message consumption rate
   - Processing status distribution (DELIVERED, ROUTED, DUPLICATE, FAILED)
   - Processing duration (p50, p95, p99)
   - Cassandra write latency p95

4. **Connectors** - WhatsApp e Instagram
   - Messages sent per channel (success/failed)
   - Circuit breaker state (CLOSED/OPEN/HALF_OPEN)
   - Connector API latency (p50, p95, p99)
   - Circuit breaker transitions (visualiza quando abre/fecha)

## ğŸ“ˆ Load Testing com k6

### Testes DisponÃ­veis

| Teste | DuraÃ§Ã£o | Carga | PropÃ³sito |
|-------|---------|-------|-----------|
| **baseline.js** | 5 min | 100 RPS | Estabelecer baseline de performance |
| **spike.js** | 2 min | 0â†’1000 RPS | Validar comportamento em picos de trÃ¡fego |
| **stress.js** | 10 min | 0â†’500 RPS gradual | Encontrar ponto de degradaÃ§Ã£o |
| **soak.js** | 30 min | 200 RPS constante | Detectar memory leaks e estabilidade |
| **breakpoint.js** | VariÃ¡vel | AtÃ© falhar | Encontrar RPS mÃ¡ximo absoluto |
| **file-upload.js** | 5 min | 50 RPS | Performance com uploads de 1MB |
| **mixed-workload.js** | 10 min | 150 RPS | 80% texto + 20% arquivos (cenÃ¡rio real) |

### Executar Testes

```bash
# Baseline (recomendado para comeÃ§ar)
k6 run scripts/load-tests/baseline.js

# Spike test
k6 run scripts/load-tests/spike.js

# Stress test (encontra limites)
k6 run scripts/load-tests/stress.js

# Soak test (30 minutos - detecta memory leaks)
k6 run scripts/load-tests/soak.js

# Breakpoint (encontra RPS mÃ¡ximo)
k6 run scripts/load-tests/breakpoint.js

# File upload test
k6 run scripts/load-tests/file-upload.js

# Mixed workload (cenÃ¡rio realista)
k6 run scripts/load-tests/mixed-workload.js
```

### Interpretar Resultados

```bash
# ApÃ³s cada teste, k6 exibe:
# - Total de requisiÃ§Ãµes
# - RPS mÃ©dio e mÃ¡ximo
# - LatÃªncia (p50, p95, p99, max)
# - Taxa de erro
# - Checks que passaram/falharam

# Exemplo de saÃ­da:
# âœ“ status is 200
# âœ“ response time < 200ms
# 
# http_req_duration..........: avg=85.2ms  p95=145ms  p99=180ms
# http_reqs..................: 30000 (100/s)
# errors.....................: 0.01% âœ“ (3 requests failed)
```

### Thresholds (CritÃ©rios de Sucesso)

Cada teste define thresholds para validar performance:

```javascript
thresholds: {
  'http_req_duration': ['p(95)<200'],  // 95% das requests < 200ms
  'errors': ['rate<0.01'],             // Taxa de erro < 1%
  'http_req_failed': ['rate<0.01'],    // Falhas < 1%
}
```

Se algum threshold falhar, k6 retorna exit code 1.

## ğŸ”§ Circuit Breaker

### ImplementaÃ§Ã£o

Ambos conectores (WhatsApp e Instagram) implementam o padrÃ£o Circuit Breaker para prevenir cascata de falhas:

**Estados:**
- **CLOSED** - Normal, todas as requisiÃ§Ãµes passam
- **OPEN** - Falhas detectadas, requisiÃ§Ãµes rejeitadas imediatamente (fail-fast)
- **HALF_OPEN** - Testando recuperaÃ§Ã£o, permite 1 requisiÃ§Ã£o de teste

**TransiÃ§Ãµes:**
```
CLOSED â”€â”€(5 falhas consecutivas)â”€â”€â–¶ OPEN
OPEN â”€â”€(30 segundos timeout)â”€â”€â–¶ HALF_OPEN
HALF_OPEN â”€â”€(sucesso)â”€â”€â–¶ CLOSED
HALF_OPEN â”€â”€(falha)â”€â”€â–¶ OPEN
```

**ConfiguraÃ§Ã£o:**
- `failureThreshold`: 5 falhas consecutivas abrem o circuito
- `recoveryTimeout`: 30 segundos antes de tentar HALF_OPEN
- `simulatedFailureRate`: 10% (apenas para testes)

### Monitoramento

```promql
# Ver estado atual do circuit breaker (0=CLOSED, 1=OPEN, 50=HALF_OPEN)
circuit_breaker_state{channel="whatsapp"}
circuit_breaker_state{channel="instagram"}

# Contar transiÃ§Ãµes de estado
rate(circuit_breaker_transitions_total[5m])

# Detectar quando o circuit breaker abre
changes(circuit_breaker_state{channel="whatsapp"}[5m]) > 0 and circuit_breaker_state{channel="whatsapp"} == 1
```

### CÃ³digo

LocalizaÃ§Ã£o: `connector-*/src/main/java/.../CircuitBreaker.java`

```java
public boolean allowRequest() {
    synchronized (this) {
        if (state == State.OPEN) {
            if (System.currentTimeMillis() - lastFailureTime >= RECOVERY_TIMEOUT_MS) {
                transitionTo(State.HALF_OPEN);
                return true; // Permite request de teste
            }
            return false; // Rejeita request (fail-fast)
        }
        return true; // CLOSED ou HALF_OPEN permitem
    }
}
```

## ğŸš¨ Alertas Prometheus

### ConfiguraÃ§Ã£o de Alertas

Crie arquivo `monitoring/alerts.yml`:

```yaml
groups:
  - name: chat4all_alerts
    interval: 30s
    rules:
      # High Error Rate
      - alert: HighErrorRate
        expr: |
          rate(http_requests_total{status=~"5.."}[5m]) 
          / rate(http_requests_total[5m]) > 0.05
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High error rate on {{ $labels.service }}"
          description: "Error rate is {{ $value | humanizePercentage }}"

      # High Latency
      - alert: HighLatency
        expr: |
          histogram_quantile(0.95, 
            rate(http_request_duration_seconds_bucket[5m])
          ) > 0.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High latency on {{ $labels.service }}"
          description: "P95 latency is {{ $value }}s"

      # Circuit Breaker Open
      - alert: CircuitBreakerOpen
        expr: circuit_breaker_state > 0.5
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Circuit breaker OPEN on {{ $labels.channel }}"
          description: "Connector {{ $labels.channel }} circuit breaker is OPEN"

      # High Consumer Lag
      - alert: HighConsumerLag
        expr: kafka_consumer_lag > 1000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High Kafka consumer lag"
          description: "Consumer lag is {{ $value }} messages"

      # Memory Usage High
      - alert: HighMemoryUsage
        expr: |
          jvm_memory_used_bytes{area="heap"} 
          / jvm_memory_max_bytes{area="heap"} > 0.90
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on {{ $labels.service }}"
          description: "Heap usage is {{ $value | humanizePercentage }}"

      # Service Down
      - alert: ServiceDown
        expr: up{job=~"api-service|router-worker|connector-.*"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "Prometheus cannot scrape {{ $labels.job }}"
```

### Adicionar Alertas ao Prometheus

Edite `monitoring/prometheus.yml`:

```yaml
rule_files:
  - "alerts.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets: ['alertmanager:9093']  # Opcional
```

### Testar Alertas

```bash
# Ver alertas ativos
curl http://localhost:9090/api/v1/alerts

# Ou na UI Prometheus: http://localhost:9090/alerts
```

## ğŸ” Troubleshooting

### 1. MÃ©tricas nÃ£o aparecem no Prometheus

**Sintoma:** Prometheus scrape failing ou mÃ©tricas ausentes

**DiagnÃ³stico:**
```bash
# Verificar targets no Prometheus
curl http://localhost:9090/api/v1/targets | jq '.data.activeTargets[] | {job, health, lastError}'

# Testar endpoint diretamente
curl http://localhost:8080/actuator/prometheus
```

**SoluÃ§Ãµes:**
- Verificar se serviÃ§o estÃ¡ rodando: `docker ps`
- Verificar logs: `docker logs api-service`
- Validar `prometheus.yml` scrape config
- Confirmar porta correta no `scrape_configs`

### 2. Grafana nÃ£o conecta ao Prometheus

**Sintoma:** "Data source not found" ou "Failed to fetch"

**DiagnÃ³stico:**
```bash
# Testar conectividade do container Grafana
docker exec grafana wget -O- http://prometheus:9090/api/v1/query?query=up

# Ver logs do Grafana
docker logs grafana
```

**SoluÃ§Ãµes:**
- Confirmar Prometheus rodando: `curl http://localhost:9090`
- Verificar datasource em Grafana: Settings â†’ Data sources â†’ Prometheus
- URL deve ser `http://prometheus:9090` (nome do container, nÃ£o localhost)
- Restart Grafana: `docker-compose restart grafana`

### 3. Dashboards vazios

**Sintoma:** PainÃ©is mostram "No data" ou queries retornam vazio

**DiagnÃ³stico:**
```bash
# Verificar se mÃ©tricas existem no Prometheus
curl 'http://localhost:9090/api/v1/query?query=up'

# Verificar time range no dashboard (canto superior direito)
# Verificar se serviÃ§os estÃ£o gerando trÃ¡fego
```

**SoluÃ§Ãµes:**
- Gerar trÃ¡fego: `k6 run scripts/load-tests/baseline.js`
- Ajustar time range: "Last 15 minutes" â†’ "Last 5 minutes"
- Verificar se Prometheus estÃ¡ scraping: http://localhost:9090/targets
- Confirmar query syntax no painel: Edit â†’ Query inspector

### 4. Circuit breaker sempre OPEN

**Sintoma:** `circuit_breaker_state` sempre 1 (OPEN)

**DiagnÃ³stico:**
```bash
# Ver logs do conector
docker logs connector-whatsapp 2>&1 | grep -i "circuit"

# Verificar transiÃ§Ãµes de estado
curl 'http://localhost:9090/api/v1/query?query=circuit_breaker_transitions_total'
```

**SoluÃ§Ãµes:**
- Confirmar que serviÃ§o externo estÃ¡ acessÃ­vel
- Verificar logs: `docker logs connector-whatsapp --tail 50`
- Reduzir `simulatedFailureRate` em `WhatsAppConnector.java` (atualmente 10%)
- Aguardar 30 segundos (recoveryTimeout) para HALF_OPEN

### 5. k6 test falha com "Connection refused"

**Sintoma:** `ERRO[0001] GoError: dial tcp 127.0.0.1:8080: connect: connection refused`

**DiagnÃ³stico:**
```bash
# Verificar se API estÃ¡ rodando
curl http://localhost:8080/health

# Verificar portas
docker ps | grep 8080
```

**SoluÃ§Ãµes:**
- Iniciar stack: `docker-compose up -d`
- Aguardar health checks: `./scripts/demo-observability.sh`
- Confirmar porta correta no script k6
- Verificar firewall/iptables nÃ£o estÃ¡ bloqueando

### 6. Consumer lag crescendo infinitamente

**Sintoma:** `kafka_consumer_lag` aumenta continuamente

**DiagnÃ³stico:**
```bash
# Verificar taxa de consumo vs produÃ§Ã£o
curl 'http://localhost:9090/api/v1/query?query=rate(messages_consumed_total[5m])'
curl 'http://localhost:9090/api/v1/query?query=rate(messages_accepted_total[5m])'

# Ver logs do router-worker
docker logs router-worker --tail 100
```

**SoluÃ§Ãµes:**
- Escalar router-worker: `docker-compose up -d --scale router-worker=3`
- Verificar se Cassandra estÃ¡ respondendo: `docker logs cassandra`
- Reduzir carga de entrada (parar k6 tests)
- Aumentar `KAFKA_CONSUMER_THREADS` em `router-worker/Dockerfile`

### 7. Grafana dashboard nÃ£o salva

**Sintoma:** "Dashboard not found" apÃ³s refresh

**DiagnÃ³stico:**
```bash
# Verificar volume Grafana
docker volume ls | grep grafana

# Ver logs
docker logs grafana --tail 50
```

**SoluÃ§Ãµes:**
- Dashboards em `monitoring/grafana/dashboards/` sÃ£o read-only (provisioned)
- Para editar: salve uma cÃ³pia com novo nome
- Ou edite JSON diretamente e reinicie: `docker-compose restart grafana`

## ğŸ“Š Queries PromQL Ãšteis

### Performance Metrics

```promql
# Request rate por segundo
rate(http_requests_total[5m])

# LatÃªncia p95 (Ãºltima hora)
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[1h]))

# Taxa de erro (Ãºltimos 5 min)
rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m])

# Throughput total (messages/sec)
sum(rate(messages_accepted_total[5m]))
```

### Resource Utilization

```promql
# CPU usage (%)
process_cpu_usage * 100

# Heap memory usage (%)
jvm_memory_used_bytes{area="heap"} / jvm_memory_max_bytes{area="heap"} * 100

# GC time (% of time in GC)
rate(jvm_gc_pause_seconds_sum[5m]) / rate(jvm_gc_pause_seconds_count[5m])
```

### Kafka Metrics

```promql
# Consumer lag total
sum(kafka_consumer_lag)

# Consumption rate (messages/sec)
rate(messages_consumed_total[5m])

# Kafka publish latency p99
histogram_quantile(0.99, rate(kafka_publish_duration_seconds_bucket[5m]))
```

### Circuit Breaker Metrics

```promql
# Estado atual (0=CLOSED, 1=OPEN, 50=HALF_OPEN)
circuit_breaker_state

# Taxa de transiÃ§Ãµes (transiÃ§Ãµes/min)
rate(circuit_breaker_transitions_total[5m]) * 60

# Tempo em estado OPEN (segundos)
time() - (circuit_breaker_state > 0) * time()
```

## ğŸ§ª Testes de ValidaÃ§Ã£o

### Smoke Test (ValidaÃ§Ã£o RÃ¡pida)

```bash
#!/bin/bash
# scripts/smoke-test-observability.sh

echo "=== Smoke Test - Observability ==="

# 1. Verificar Prometheus up
if curl -sf http://localhost:9090/-/healthy > /dev/null; then
  echo "âœ“ Prometheus healthy"
else
  echo "âœ— Prometheus down"; exit 1
fi

# 2. Verificar Grafana up
if curl -sf http://localhost:3000/api/health > /dev/null; then
  echo "âœ“ Grafana healthy"
else
  echo "âœ— Grafana down"; exit 1
fi

# 3. Verificar mÃ©tricas API Service
if curl -sf http://localhost:8080/actuator/prometheus | grep -q "http_requests_total"; then
  echo "âœ“ API metrics exposed"
else
  echo "âœ— API metrics missing"; exit 1
fi

# 4. Verificar mÃ©tricas Router Worker
if curl -sf http://localhost:8082/actuator/prometheus | grep -q "messages_consumed_total"; then
  echo "âœ“ Router metrics exposed"
else
  echo "âœ— Router metrics missing"; exit 1
fi

# 5. Verificar targets Prometheus
TARGETS=$(curl -sf http://localhost:9090/api/v1/targets | jq -r '.data.activeTargets[] | select(.health=="up") | .job' | wc -l)
if [ "$TARGETS" -ge 4 ]; then
  echo "âœ“ Prometheus scraping $TARGETS targets"
else
  echo "âœ— Only $TARGETS targets up (expected 4+)"; exit 1
fi

# 6. Enviar mensagem de teste
TOKEN=$(curl -sf -X POST http://localhost:8080/api/v1/auth/register \
  -H "Content-Type: application/json" \
  -d '{"userId":"smoke_test"}' | jq -r '.token')

if [ -n "$TOKEN" ]; then
  echo "âœ“ Auth token obtained"
else
  echo "âœ— Auth failed"; exit 1
fi

MSG_ID=$(curl -sf -X POST http://localhost:8080/api/v1/messages \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"content":"Smoke test","channel":"whatsapp"}' | jq -r '.id')

if [ -n "$MSG_ID" ]; then
  echo "âœ“ Message sent: $MSG_ID"
else
  echo "âœ— Message send failed"; exit 1
fi

# 7. Verificar mÃ©trica foi incrementada
sleep 2
COUNT=$(curl -sf 'http://localhost:9090/api/v1/query?query=messages_accepted_total' | jq -r '.data.result[0].value[1]')
if [ "$COUNT" -gt 0 ]; then
  echo "âœ“ Metrics updated (messages_accepted_total=$COUNT)"
else
  echo "âœ— Metrics not updating"; exit 1
fi

echo ""
echo "=== âœ“ All smoke tests passed ==="
```

### Executar Smoke Test

```bash
chmod +x scripts/smoke-test-observability.sh
./scripts/smoke-test-observability.sh
```

## ğŸ“š ReferÃªncias

### DocumentaÃ§Ã£o Oficial
- [Prometheus](https://prometheus.io/docs/)
- [Grafana](https://grafana.com/docs/)
- [k6](https://k6.io/docs/)
- [Micrometer](https://micrometer.io/docs)

### PadrÃµes e Best Practices
- [Circuit Breaker Pattern - Martin Fowler](https://martinfowler.com/bliki/CircuitBreaker.html)
- [Google SRE Book - Monitoring](https://sre.google/sre-book/monitoring-distributed-systems/)
- [RED Method](https://grafana.com/blog/2018/08/02/the-red-method-how-to-instrument-your-services/) - Rate, Errors, Duration
- [USE Method](http://www.brendangregg.com/usemethod.html) - Utilization, Saturation, Errors

### Arquivos do Projeto
- ConfiguraÃ§Ã£o Prometheus: `monitoring/prometheus.yml`
- Dashboards Grafana: `monitoring/grafana/dashboards/*.json`
- Load tests k6: `scripts/load-tests/*.js`
- Demo script: `scripts/demo-observability.sh`
- Circuit Breaker: `connector-*/src/main/java/.../CircuitBreaker.java`

---

**Status**: âœ… ImplementaÃ§Ã£o completa - Entrega 3  
**Ãšltima atualizaÃ§Ã£o**: 2024

**Sintoma:** "Data source not found" ou "Failed to fetch"

**DiagnÃ³stico:**
```bash
# Testar conectividade do container Grafana
docker exec grafana wget -O- http://prometheus:9090/api/v1/query?query=up

# Ver logs do Grafana
docker logs grafana
```

**SoluÃ§Ãµes:**
- Confirmar Prometheus rodando: `curl http://localhost:9090`
- Verificar datasource em Grafana: Settings â†’ Data sources â†’ Prometheus
- URL deve ser `http://prometheus:9090` (nome do container, nÃ£o localhost)
- Restart Grafana: `docker-compose restart grafana`

### 3. Dashboards vazios
  - LatÃªncia HTTP (P50, P95, P99)
  - Error rate (%)
  - Top 10 conversas por volume

### Dashboard 2: Kafka & Event Processing
- **Objetivo**: Monitorar barramento de eventos
- **PainÃ©is**:
  - Messages published vs consumed
  - Consumer lag por partition
  - Tempo de processamento do Router Worker
  - Total de mensagens pendentes

### Dashboard 3: Infrastructure Health
- **Objetivo**: SaÃºde da infraestrutura
- **PainÃ©is**:
  - CPU usage por container
  - Memory usage por container
  - Cassandra write latency
  - MinIO bandwidth (upload/download)
  - Health check status (verde/vermelho)

### Dashboard 4: Business Metrics
- **Objetivo**: MÃ©tricas de negÃ³cio
- **PainÃ©is**:
  - DistribuiÃ§Ã£o de mensagens por status (SENT/DELIVERED/READ)
  - Top canais de entrega (WhatsApp vs Instagram)
  - Files uploaded (count + total size)
  - Ãšltimas mensagens com file_id (real-time)

> ğŸš§ **Dashboards em Desenvolvimento** - Os JSON files serÃ£o adicionados em `monitoring/grafana/dashboards/`

## ğŸ” Queries Prometheus Ãšteis

### Validar Requisitos de Performance

```promql
# Throughput: Mensagens aceitas por segundo
rate(messages_accepted_total[1m])

# LatÃªncia P95: DuraÃ§Ã£o de requisiÃ§Ãµes HTTP (95Âº percentil)
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))

# LatÃªncia P99
histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))

# Error rate: Percentual de erros 5xx
rate(http_requests_by_status_total{status=~"5.."}[1m]) 
/ 
rate(http_requests_total[1m]) * 100

# Tempo mÃ©dio de publicaÃ§Ã£o no Kafka
rate(kafka_publish_duration_seconds_sum[5m]) 
/ 
rate(kafka_publish_duration_seconds_count[5m])

# Arquivos grandes (>100MB) enviados por hora
increase(files_uploaded_by_size_total{size_bucket="xlarge"}[1h])
```

### Monitorar SaÃºde do Sistema

```promql
# Memory usage por serviÃ§o
jvm_memory_used_bytes{service="api-service", area="heap"}

# CPU usage (processo Java)
process_cpu_usage{service="api-service"}

# Threads ativas
jvm_threads_live{service="api-service"}

# GC pauses longas (> 100ms)
histogram_quantile(0.99, rate(jvm_gc_pause_seconds_bucket[5m])) > 0.1
```

## ğŸ”§ Health Checks

### Liveness Check (Simple)
```bash
# Endpoint: GET /health
# PropÃ³sito: Verificar se o serviÃ§o estÃ¡ vivo
# Uso: Docker healthcheck, Kubernetes liveness probe

curl http://localhost:8080/health
# Response: {"status":"UP"}
```

### Readiness Check (Detailed)
```bash
# Endpoint: GET /actuator/health
# PropÃ³sito: Verificar se o serviÃ§o estÃ¡ pronto para receber trÃ¡fego
# Uso: Kubernetes readiness probe, load balancer checks

curl http://localhost:8080/actuator/health
# Response:
{
  "status": "UP",
  "components": {
    "kafka": {
      "status": "UP",
      "latency_ms": 5
    },
    "cassandra": {
      "status": "UP",
      "latency_ms": 12
    }
  },
  "timestamp": "2025-11-26T21:30:45Z",
  "check_duration_ms": 18
}
```

### Docker Healthcheck
```yaml
# Configurado em docker-compose.yml
healthcheck:
  test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 40s
```

## ğŸ“Š ValidaÃ§Ã£o de SLA

### Requisitos (esqueleto.md - SeÃ§Ã£o 1.1)

| Requisito | MÃ©trica | Query Prometheus | Target |
|-----------|---------|------------------|--------|
| **Throughput** | Mensagens/minuto | `rate(messages_accepted_total[1m]) * 60` | â‰¥ 10,000 |
| **LatÃªncia P95** | HTTP request duration | `histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))` | < 0.2s (200ms) |
| **Error Rate** | % de erros | `rate(http_requests_by_status_total{status=~"5.."}[1m]) / rate(http_requests_total[1m]) * 100` | < 0.1% |
| **Disponibilidade** | Uptime | `up{job="api-service"}` | > 99.9% |

### Comandos de VerificaÃ§Ã£o

```bash
# Verificar throughput atual
curl -s 'http://localhost:9090/api/v1/query?query=rate(messages_accepted_total[1m])*60' | jq '.data.result[0].value[1]'

# Verificar P95 latency
curl -s 'http://localhost:9090/api/v1/query?query=histogram_quantile(0.95,rate(http_request_duration_seconds_bucket[5m]))' | jq '.data.result[0].value[1]'

# Verificar error rate
curl -s 'http://localhost:9090/api/v1/query?query=rate(http_requests_by_status_total{status=~"5.."}[1m])/rate(http_requests_total[1m])*100' | jq '.data.result[0].value[1]'
```

## ğŸ› Troubleshooting

### Prometheus nÃ£o estÃ¡ coletando mÃ©tricas

```bash
# 1. Verificar se Prometheus estÃ¡ rodando
docker ps | grep prometheus

# 2. Verificar targets
curl http://localhost:9090/api/v1/targets | jq '.data.activeTargets[] | {job, health, lastError}'

# 3. Verificar se o endpoint estÃ¡ acessÃ­vel
curl http://api-service:8080/actuator/prometheus

# 4. Verificar logs do Prometheus
docker logs chat4all-prometheus
```

### Grafana nÃ£o mostra dados

```bash
# 1. Verificar datasource
curl -u admin:admin http://localhost:3000/api/datasources | jq '.[].name'

# 2. Testar query no Prometheus
curl 'http://localhost:9090/api/v1/query?query=up'

# 3. Verificar logs do Grafana
docker logs chat4all-grafana
```

### MÃ©tricas nÃ£o aparecem

```bash
# 1. Verificar se o endpoint existe
curl -v http://localhost:8080/actuator/prometheus 2>&1 | grep "HTTP/"

# 2. Verificar se hÃ¡ mÃ©tricas sendo geradas
# Enviar algumas mensagens e verificar
curl http://localhost:8080/actuator/prometheus | grep messages_accepted_total

# 3. Verificar logs da aplicaÃ§Ã£o
docker logs api-service 2>&1 | grep -i metric
```

## ğŸ“š ReferÃªncias

- **Arquitetura**: `esqueleto.md` - SeÃ§Ã£o 2.4 (Observabilidade)
- **Requisitos**: `esqueleto.md` - SeÃ§Ã£o 1.1 (Requisitos CrÃ­ticos de Performance)
- **ImplementaÃ§Ã£o**: `entrega3.md` - Atividade 1 (Implementar Stack de Observabilidade)
- **Prometheus**: https://prometheus.io/docs/
- **Grafana**: https://grafana.com/docs/
- **Micrometer**: https://micrometer.io/docs/

## âœ… Status de ImplementaÃ§Ã£o

- [x] Prometheus configurado no docker-compose.yml
- [x] Grafana configurado no docker-compose.yml
- [x] prometheus.yml com targets dos microsserviÃ§os
- [x] Datasource Prometheus no Grafana (auto-provisioning)
- [x] API Service: endpoint /actuator/prometheus
- [x] API Service: mÃ©tricas HTTP, mensagens, arquivos, Kafka
- [x] API Service: health checks (/health e /actuator/health)
- [x] MetricsInterceptor: captura automÃ¡tica de mÃ©tricas HTTP
- [ ] Router Worker: endpoint /actuator/prometheus
- [ ] Connectors: endpoint /actuator/prometheus
- [ ] Dashboards Grafana (JSON files)
- [ ] Alertas configurados
- [ ] DocumentaÃ§Ã£o de queries avanÃ§adas

---

**Chat4All - Observabilidade**  
**Status**: ğŸ”„ Parcialmente Implementado (60%)  
**PrÃ³ximo**: Implementar mÃ©tricas no Router Worker e Connectors
